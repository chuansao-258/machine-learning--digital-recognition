\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\title{Digital recognization by five methods}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
	Lanhe Gao 2018533212\\
	SIST\\
	ShanghaiTech University\\
	Shanghai, China \\
	\texttt{gaolh@shanghaitech.edu.cn} \\
	\And
	Yunfei Zhang 2018533098\\
	SIST\\
	ShanghaiTech University\\
	Shanghai, China \\
	\texttt{zhanyf2@shanghaitech.edu.cn} \\
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Recognizing handwritten numbers is a very classical machine learning problem, which has a variety of solutions. In this report, we try to implement five of these methods. By comparing their performances, we hope to choose the best one.
	\end{abstract}

	\section{Introduction}
	
	Number recognization is a widely used application, with great sophistication. There are many types of machine learning methods that can be used in this scenario. In this report, we have adopted five methods, such as Support Vector Machine (SVM), K-Nearsest Neighbors (KNN), Naive Bayes (NB), Neural Networks (NN) and Logistic Regression (LR). We will analyze the accuracy, runtime performance, confusion matrix and other standards to compare their performance, and conclude the best method.
	
	\section{Data Setting and Methods}
	
	\subsection{Data setting}
	
	Our training data and testing data are collected from \textit{mnist} dataset. They're in the same form, with 28*28 pixels, therefore, we use a 784 dimensional vector to represent each number picture. The pictures are gray images, scaling from 0 to 255.
	
	\subsection{KNN}
	
	The first method we use is KNN. We convert the images to a 784 dimensional vector for easier calculation. Since the dimension is quite big, which makes the complexity of calculating 2-norm quite big, so we use 1-norm instead. KNN calculates K nearest images that has the smalles 1-norm distance to a given test data, and labels it as the class that contains the most number of neighbors. \\ \\
	The code implementation is at 
	\begin{center}
		\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/knn.ipynb}
	\end{center}
	
	\subsection{SVM}
	
	We use the svm from \textit{sklearn} package. We directly put the data and label into the model \textit{svm.fit()} for training, then use \textit{svm.predict()} to run prediction. \\ \\
	The code implementation is at
	\begin{center}
		\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/SVM.ipynb}
	\end{center}
	
	\subsection{NB}
	
	 Before coding, we tried the naive bayes model in sklearn. However, it does not work well, with only 58\% accuracy, so we try to implement one by ourselves. For training data, we calculate two thing: the probabilities of an image being labeled as each class; and the probabilities of the gray value of a pixel being $0$ in each class, which means we choose every pixel as a feature. \\ \\
	 Then, the prediction works like this: given a test image, we calculate the probability multiplication of all pixel points, and choose the label with the highest probability. \\ \\
	 The code implementation is at 
	 \begin{center}
	 	\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/bayes.ipynb}
	 \end{center}
 	 
 	 \subsection{LR}
 	 
 	 As a matter of fact, Logistic Regression is a binary classification method. However, we want to classify these handwritings into 10 classes, so we need to extend LR method. For each class, we devide the dataset into the ones belong to the class, and the ones do not. This is a binary classification problem, and we need to do it 10 times. Finally, we choose the class that the input has the highest probability on.
 	 
 	 \subsection{NN}
	 
	 \section{Results}
	 
	 \subsection{Accuracy}
	 
	 As displayed in the jupyter notebook, the accuracy of these methods are
	 \begin{table}[htbp]
	 	\centering  % 显示位置为中间
	 	\caption{Accuracy table}  % 表格标题
	 	\label{table1}  % 用于索引表格的标签
	 	%字母的个数对应列数，|代表分割线
	 	% l代表左对齐，c代表居中，r代表右对齐
	 	\begin{tabular}{|c|c|}  
	 		\hline  % 表格的横线
	 		& \\[-6pt]  %可以避免文字偏上来调整文字与上边界的距离
	 		KNN & 47\% \\  % 表格中的内容，用&分开，\\表示下一行
	 		\hline
	 		& \\[-6pt]  %可以避免文字偏上 
	 		NB& 68.41\% \\
	 		\hline
	 		& \\[-6pt]
	 		SVM & 72.92\%\\
	 		\hline
	 	\end{tabular}
	 \end{table}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	

	
\end{document}