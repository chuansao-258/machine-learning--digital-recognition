\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}

\title{Digital recognization by five methods}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
	Lanhe Gao 2018533212\\
	SIST\\
	ShanghaiTech University\\
	Shanghai, China \\
	\texttt{gaolh@shanghaitech.edu.cn} \\
	\And
	Yunfei Zhang 2018533098\\
	SIST\\
	ShanghaiTech University\\
	Shanghai, China \\
	\texttt{zhanyf2@shanghaitech.edu.cn} \\
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Recognizing handwritten numbers is a very classical machine learning problem, which has a variety of solutions. In this report, we try to implement five of these methods. By comparing their performances, we hope to choose the best one.
	\end{abstract}

	\section{Introduction}
	
	Number recognization is a widely used application, with great sophistication. There are many types of machine learning methods that can be used in this scenario. In this report, we have adopted five methods, such as Support Vector Machine (SVM), K-Nearsest Neighbors (KNN), Naive Bayes (NB), Neural Networks (NN) and Logistic Regression (LR). We will analyze the accuracy, runtime performance, confusion matrix and other standards to compare their performance, and conclude the best method.
	
	\section{Data Setting and Methods}
	
	\subsection{Data setting}
	
	Our training data and testing data are collected from \textit{mnist} dataset. They're in the same form, with 28*28 pixels, therefore, we use a 784 dimensional vector to represent each number picture. The pictures are gray images, scaling from 0 to 255.
	
	\subsection{KNN}
	
	The first method we use is KNN. Consider that each picture is composed of 784 pixels. If we expand it into a 784-dimensional vector in a fixed spatial order, we can put each test point into a hyperplane. Since the dimension is quite big, which makes the complexity of calculating 2-norm quite big, so we use 1-norm instead. KNN calculates K nearest images that has the smalles 1-norm distance to a given test data, and labels it as the class that contains the most number of neighbors. 
	
	It takes at least $O(784)$ time to calculate the distance for every two pictures. The number of samples in the training set is 60,000, the number of samples in the test set is 10,000, and the time responsibility is $O(5*10^11)$, which is too long. In our test, we select the first 100 test set samples for testing, which approximates the matching rate of the KNN algorithm.
	
	The code implementation is at 
	\begin{center}
		\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/knn.ipynb}
	\end{center}
	
	\subsection{SVM}
	
	The SVM algorithm was originally designed for binary classification problems. When dealing with multi-class problems, it is necessary to construct a suitable multi-class classifier. So when extracting the training set, take class $0$ as an example, extract:
	
	\begin{equation}
		+: 0, -: 1\sim 8
	\end{equation}

	Other class work similarly. When testing, use the ten training result files to test the corresponding test vectors. Finally, each test has a result $f_0(x), f_1(x), \cdots, f_{9}(x)$. The final result is the largest of these four values as the classification result.
	
	We use the svm from \textit{sklearn} package. We directly put the data and label into the model \textit{svm.fit()} for training, then use \textit{svm.predict()} to run prediction. The code implementation is at
	\begin{center}
		\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/SVM.ipynb}
	\end{center}
	
	\subsection{NB}
	
	In Bayes model. we assume all features are independent effects of the label. Simple digit recognition version: One feature(variable) $F_{i, j}$ for each grid position $<i,j>$. Feature values are on/off, based on whether intensity is more or less than 0.5 in underlying image. Each input maps to a feature vector, e.g. Image1 $\rightarrow (F_{0, 0} = 0,  F_{0, 1}  = 0,  F_{0, 2}  = 1,..., F_{27, 27}  = 0)$. 
	
	 Then, the prediction works like this: given a test image, we calculate the probability multiplication of all pixel points, and choose the label with the highest probability, As shown in Figure \ref{bayes}.
	 
	 \begin{figure}[htbp]
	 	\centering
	 	\includegraphics[scale=0.6]{1.png}
	 	\caption{Bayes classification}
	 	\label{bayes}
	 \end{figure}
	 
	 Before coding, we tried the naive bayes model in sklearn. However, it does not work well, with only 58\% accuracy, so we try to implement one by ourselves. The code implementation is at 
	 \begin{center}
	 	\url{https://github.com/chuansao-258/machine-learning--digital-recognition/blob/main/bayes.ipynb}
	 \end{center}
 	 
 	 \subsection{LR}
 	 
 	 Similar to SVM, Logistic Regression is also a binary classification method. However, we want to classify these handwritings into 10 classes, so we need to extend LR method as well. For each class, we devide the dataset into the ones belong to the class, and the ones do not. For example, from class $0$'s perspective, the dataset would be labeled as:
 	 
 	 \begin{equation}
 	 	+: 0, -: 1\sim 8
 	 \end{equation}
  
	This turns out to be a binary classification problem now, and we need to do it for every class, 10 times in total. Finally, we choose the class that the input has the highest probability on.
 	 
 	 \subsection{NN}
	 
	 \section{Results}
	 
	 \subsection{Accuracy}
	 
	 As displayed in the jupyter notebook, the accuracy of these methods are
	 \begin{table}[htbp]
	 	\centering  % 显示位置为中间
	 	\caption{Accuracy table}  % 表格标题
	 	\label{table1}  % 用于索引表格的标签
	 	%字母的个数对应列数，|代表分割线
	 	% l代表左对齐，c代表居中，r代表右对齐
	 	\begin{tabular}{|c|c|}  
	 		\hline  % 表格的横线
	 		& \\[-6pt]  %可以避免文字偏上来调整文字与上边界的距离
	 		KNN & 47\% \\  % 表格中的内容，用&分开，\\表示下一行
	 		\hline
	 		& \\[-6pt]  %可以避免文字偏上 
	 		NB& 68.41\% \\
	 		\hline
	 		& \\[-6pt]
	 		SVM & 72.92\%\\
	 		\hline
	 	\end{tabular}
	 \end{table}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	

	
\end{document}